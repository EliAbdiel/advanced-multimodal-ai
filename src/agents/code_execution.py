import chainlit as cl

# from google.genai import types
# from google.genai.types import GenerateContentResponse
from src.utils.llm_setup import get_gemini_client, get_llm_coder
from src.utils.config import GEMINI_2_5_MODEL, PROMPT_CODER
# from langchain_core.messages import HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate
from langchain_core.tools import tool

@tool()
async def code_generation(user_message: str) -> str:
    """
    AI-powered code generation assistant for creating and explaining code snippets.
    
    This tool leverages a specialized coding LLM to:
    1. Generate code in various programming languages based on natural language descriptions.
    2. Provide explanations for the generated code.
    3. Act as an expert software developer for coding-related queries.
    
    Workflow:
    1. Initializes a specialized coding LLM instance via `get_llm_coder`.
    2. Constructs a prompt using a system message that primes the LLM as a coding expert.
    3. Invokes the model with the user's request.
    4. Returns the raw code or explanation generated by the model.
    
    Args:
    -----------
    user_message : str
        A natural language prompt describing the desired code or programming question.
        (e.g., "Write a python script to sort a list of dictionaries by a specific key")
    
    Returns:
    --------
    str
        The generated code snippet, often formatted in a markdown block, or a textual explanation.
    
    Technical Details:
    -----------------
    - Uses a dedicated coding model for high-quality code generation.
    - Employs a `ChatPromptTemplate` with a system prompt (`PROMPT_CODER`) to ensure the model
      adopts the persona of an expert software developer.
    - Asynchronously invokes the model for non-blocking execution.
    
    Example Usage:
    -------------
    Input: "Create a javascript function that fetches data from an API and handles errors."
    Output: "
    ```javascript
    async function fetchData(url) {
      // ... function implementation ...
    }
    ```
    "
    """
    await cl.Message(content="Code execution Selected!\nPlease wait while I work on it!").send()

    coder = await get_llm_coder()

    prompt_coder = ChatPromptTemplate.from_messages(
        [
            ("system", PROMPT_CODER), 
            ("user", "{text}")
        ]
    )

    prompt = await prompt_coder.ainvoke({"text": user_message})

    response = await coder.ainvoke(prompt)

    return response.content

    # input = ("What is the sum of the first 50 prime numbers? "
    #         "Generate and run code for the calculation, and make sure you get all 50.")

    # system_instruction = """
    #         You are an expert software developer and a helpful coding assistant.
    #         You are able to generate high-quality code in any programming language.
    #     """

    # chat = client.chats.create(
    #     model=GEMINI_2_5_MODEL,
    #     config=types.GenerateContentConfig(
    #         system_instruction=system_instruction,
    #         tools=[types.Tool(code_execution=types.ToolCodeExecution)]
    #     ),
    # )

    # response = chat.send_message(user_message)

    # return await get_response(response)

# async def get_response(response: GenerateContentResponse) -> str:
#     """Processes the response from the Gemini model to extract text, code, and output."""
#     text = ""
#     code = ""
#     output = ""

#     for part in response.candidates[0].content.parts:
#         if part.text is not None:
#             text= part.text
#         if part.executable_code is not None:
#             code= part.executable_code.code
#         if part.code_execution_result is not None:
#             output = part.code_execution_result.output

#     answer = f"{text}\n\n```python\n{code}\n```\n\n{output}"
#     print(f"\n{answer}\n")
#     return answer